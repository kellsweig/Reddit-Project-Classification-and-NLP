{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462c1452-f5e7-4966-9683-da5dce7acae2",
   "metadata": {},
   "source": [
    "# Data Cleaning & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b78a343-102f-4c8d-9054-0cfe136eea98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Steps:\n",
    "\n",
    "1. Read in various dataframes and append into 1 dataframe\n",
    "2. Drop duplicated values\n",
    "3. Handle missing values (missing text)\n",
    "4. Handle links & short 'posts'\n",
    "5. Countvectorize (text & title?- combine both or keep separate?) \n",
    "6. Examine trends (average word-count, sentiment analysis etc)\n",
    "7. Store dataframe with new columns etc (how large will this be? Could that be a problem?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cac81797-cad8-475a-8b51-349ce89d27ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b43587ff-9eb0-4d8d-a3ec-5a1e0444db0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u1 = pd.read_csv('data/Ultralight/ultralight.csv')\n",
    "u2 = pd.read_csv('data/Ultralight/ultralight1.csv')\n",
    "u3 = pd.read_csv('data/Ultralight/ultralight2.csv')\n",
    "u4 = pd.read_csv('data/Ultralight/ultralight3.csv')\n",
    "u5 = pd.read_csv('data/Ultralight/ultralight4.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9ce85b7f-35f6-42cf-8111-51c6ac4f2899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = [u1, u2, u3, u4, u5]\n",
    "ultralight = pd.concat(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c1a42d6-53b0-4da5-a195-f30052d9f9a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9445, 5)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultralight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "14200ba9-62f2-429f-9b6d-7b78497006d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     3974\n",
       "created_utc    2001\n",
       "title          1996\n",
       "self_text      2584\n",
       "subreddit         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultralight.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d01b0411-d097-4a32-8f92-5ed86f89d709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultralight.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3621590-52a2-45eb-81c7-8023b7b85b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ultralight.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "57c6e8ec-28d3-433c-b9e0-5c82d2276bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9438, 5)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultralight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e6be3dee-0a01-4a50-ba7a-090fc3ea243f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c1 = pd.read_csv('data/Camping/camping.csv')\n",
    "c2 = pd.read_csv('data/Camping/camping1.csv')\n",
    "c3 = pd.read_csv('data/Camping/camping2.csv')\n",
    "c4 = pd.read_csv('data/Camping/camping3.csv')\n",
    "c5 = pd.read_csv('data/Camping/camping4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3fbf6ef3-ea7b-4053-b64f-0fda0c694544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cdatasets= [c1, c2, c3, c4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2f2b80d3-3d70-4f73-8799-8e2187f4ba01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "camping = pd.concat(cdatasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f74af42f-791c-4226-a796-66c55f6381d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8609, 5)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "123f41bc-17b7-4457-a477-c7113c329acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     3832\n",
       "created_utc    2862\n",
       "title          2851\n",
       "self_text      1610\n",
       "subreddit         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camping.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "76fa1b16-2197-4fef-a6bb-a05a577baa0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1319"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camping.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8cc049f3-8b50-450f-9424-1bca155a825a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "camping.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c94b7741-226a-4ca5-894a-5dc33766685a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7290, 5)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eb26dd8e-13ce-49eb-8816-2a1998ccea57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camping.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8a210091-bb8e-4795-9b0b-58764dcb9f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        0\n",
       "created_utc       0\n",
       "title             0\n",
       "self_text      3285\n",
       "subreddit         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camping.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5e2005ca-46a4-4405-96c5-9dd420c7de93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "created_utc      0\n",
       "title            0\n",
       "self_text      459\n",
       "subreddit        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultralight.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5e428128-ae3c-4968-85c6-27da580bdc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined = [camping, ultralight]\n",
    "df = pd.concat(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3dcc9b3c-9e8f-4de6-a8be-5750c6d40614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9438\n",
       "0    7290\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Binarize Subreddit column with ultralight = 1 and camping = 0\n",
    "df['subreddit'] = df['subreddit'].replace({'Ultralight': 1, 'camping': 0})\n",
    "df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "23465990-cc48-4b06-b5d3-c36e8f21320b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16728, 5)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "590f808f-4152-4374-a26d-c9d0fc3c48ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.697508e+09</td>\n",
       "      <td>Sierras</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.697501e+09</td>\n",
       "      <td>Car camping recommendations in the GA/NC/TN area?</td>\n",
       "      <td>Hi there! Looking for solid car camping recomm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.697499e+09</td>\n",
       "      <td>Oceano Dunes, CA</td>\n",
       "      <td>Annual trip with friends &amp; family. Always an e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.697498e+09</td>\n",
       "      <td>Fort pickens, Fl</td>\n",
       "      <td>Picks from this past weekend. Would recommend ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.697498e+09</td>\n",
       "      <td>Camping groups</td>\n",
       "      <td>Are there camping groups out there, that someo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   created_utc  \\\n",
       "0           0  1.697508e+09   \n",
       "1           1  1.697501e+09   \n",
       "2           2  1.697499e+09   \n",
       "3           3  1.697498e+09   \n",
       "4           4  1.697498e+09   \n",
       "\n",
       "                                               title  \\\n",
       "0                                            Sierras   \n",
       "1  Car camping recommendations in the GA/NC/TN area?   \n",
       "2                                   Oceano Dunes, CA   \n",
       "3                                   Fort pickens, Fl   \n",
       "4                                     Camping groups   \n",
       "\n",
       "                                           self_text  subreddit  \n",
       "0                                                NaN          0  \n",
       "1  Hi there! Looking for solid car camping recomm...          0  \n",
       "2  Annual trip with friends & family. Always an e...          0  \n",
       "3  Picks from this past weekend. Would recommend ...          0  \n",
       "4  Are there camping groups out there, that someo...          0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1fb930e4-b4bc-4ded-b9b7-6fcb79551c71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3969</th>\n",
       "      <td>3969</td>\n",
       "      <td>1.685748e+09</td>\n",
       "      <td>Shakedown For Hiking on the Snowbank Trail in ...</td>\n",
       "      <td>Current base weight: 12.58\\n\\nLocation/temp ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>3970</td>\n",
       "      <td>1.685742e+09</td>\n",
       "      <td>Custom Carbon Tent Poles</td>\n",
       "      <td>Hey so, have a hexamid solo and am primarily a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>3971</td>\n",
       "      <td>1.685721e+09</td>\n",
       "      <td>GDT shakedown</td>\n",
       "      <td>I'll be hiking sections C, D, and E of the Gre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3972</th>\n",
       "      <td>3972</td>\n",
       "      <td>1.685718e+09</td>\n",
       "      <td>[Trip Report] GSMNP overnight loop. Mt. Sterli...</td>\n",
       "      <td>I went out over Memorial Day weekend for a 22m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>3973</td>\n",
       "      <td>1.685721e+09</td>\n",
       "      <td>Looking for advice for a summer quilt</td>\n",
       "      <td>Hello there,\\n\\nto make it short i want to buy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0   created_utc  \\\n",
       "3969        3969  1.685748e+09   \n",
       "3970        3970  1.685742e+09   \n",
       "3971        3971  1.685721e+09   \n",
       "3972        3972  1.685718e+09   \n",
       "3973        3973  1.685721e+09   \n",
       "\n",
       "                                                  title  \\\n",
       "3969  Shakedown For Hiking on the Snowbank Trail in ...   \n",
       "3970                           Custom Carbon Tent Poles   \n",
       "3971                                      GDT shakedown   \n",
       "3972  [Trip Report] GSMNP overnight loop. Mt. Sterli...   \n",
       "3973              Looking for advice for a summer quilt   \n",
       "\n",
       "                                              self_text  subreddit  \n",
       "3969  Current base weight: 12.58\\n\\nLocation/temp ra...          1  \n",
       "3970  Hey so, have a hexamid solo and am primarily a...          1  \n",
       "3971  I'll be hiking sections C, D, and E of the Gre...          1  \n",
       "3972  I went out over Memorial Day weekend for a 22m...          1  \n",
       "3973  Hello there,\\n\\nto make it short i want to buy...          1  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ca394ce4-a45b-4cdb-96cb-d451a3b591d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/combined_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d1f9a95d-ca56-4c65-81a3-ebd825adc9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        0\n",
       "created_utc       0\n",
       "title             0\n",
       "self_text      3744\n",
       "subreddit         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "16db4cd4-1b50-47eb-9d86-d5cae9a0a35e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       int64\n",
       "created_utc    float64\n",
       "title           object\n",
       "self_text       object\n",
       "subreddit        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d21bef77-e0bf-432c-aeb9-c8a72ff9c00c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with more than 10 words in 'self_text': 12702\n"
     ]
    }
   ],
   "source": [
    "df['word_count'] = df['self_text'].apply(lambda x: len(str(x).split()))\n",
    "count_greater_than_10 = len(df[df['word_count'] > 10])\n",
    "count_greater_than_10 = (df['word_count'] > 10).sum()\n",
    "print(\"Number of rows with more than 10 words in 'self_text':\", count_greater_than_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c8190bf8-ea7e-418b-8323-44f64b791476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['word_count_title'] = df['title'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4302b4c5-e5fc-4a21-aae6-37a099eae276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.697508e+09</td>\n",
       "      <td>Sierras</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.697501e+09</td>\n",
       "      <td>Car camping recommendations in the GA/NC/TN area?</td>\n",
       "      <td>Hi there! Looking for solid car camping recomm...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.697499e+09</td>\n",
       "      <td>Oceano Dunes, CA</td>\n",
       "      <td>Annual trip with friends &amp; family. Always an e...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.697498e+09</td>\n",
       "      <td>Fort pickens, Fl</td>\n",
       "      <td>Picks from this past weekend. Would recommend ...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.697498e+09</td>\n",
       "      <td>Camping groups</td>\n",
       "      <td>Are there camping groups out there, that someo...</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   created_utc  \\\n",
       "0           0  1.697508e+09   \n",
       "1           1  1.697501e+09   \n",
       "2           2  1.697499e+09   \n",
       "3           3  1.697498e+09   \n",
       "4           4  1.697498e+09   \n",
       "\n",
       "                                               title  \\\n",
       "0                                            Sierras   \n",
       "1  Car camping recommendations in the GA/NC/TN area?   \n",
       "2                                   Oceano Dunes, CA   \n",
       "3                                   Fort pickens, Fl   \n",
       "4                                     Camping groups   \n",
       "\n",
       "                                           self_text  subreddit  word_count  \\\n",
       "0                                                NaN          0           1   \n",
       "1  Hi there! Looking for solid car camping recomm...          0          30   \n",
       "2  Annual trip with friends & family. Always an e...          0          10   \n",
       "3  Picks from this past weekend. Would recommend ...          0          10   \n",
       "4  Are there camping groups out there, that someo...          0         276   \n",
       "\n",
       "   word_count_title  \n",
       "0                 1  \n",
       "1                 7  \n",
       "2                 3  \n",
       "3                 3  \n",
       "4                 2  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5b02012a-6602-476a-8ebe-7a53d753cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate Sentiment Intensity Analyzer\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['self_text'].fillna('', inplace=True)\n",
    "\n",
    "# Apply sentiment analysis to each row in the DataFrame\n",
    "df['sentiment'] = df['self_text'].apply(lambda x: sa.polarity_scores(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe705c-79cd-46fa-8c63-4d5188a8b265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d1372099-d35b-48db-baba-f2d4d01d3741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['vader_score'] = df['self_text'].apply(lambda x: sa.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3ef6b9dc-fc6b-4987-94df-1f224fbc86d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_title</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vader_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.697508e+09</td>\n",
       "      <td>Sierras</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.697501e+09</td>\n",
       "      <td>Car camping recommendations in the GA/NC/TN area?</td>\n",
       "      <td>Hi there! Looking for solid car camping recomm...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'comp...</td>\n",
       "      <td>0.2244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.697499e+09</td>\n",
       "      <td>Oceano Dunes, CA</td>\n",
       "      <td>Annual trip with friends &amp; family. Always an e...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.721, 'pos': 0.279, 'comp...</td>\n",
       "      <td>0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.697498e+09</td>\n",
       "      <td>Fort pickens, Fl</td>\n",
       "      <td>Picks from this past weekend. Would recommend ...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.576, 'pos': 0.424, 'comp...</td>\n",
       "      <td>0.7096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.697498e+09</td>\n",
       "      <td>Camping groups</td>\n",
       "      <td>Are there camping groups out there, that someo...</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>2</td>\n",
       "      <td>{'neg': 0.07, 'neu': 0.855, 'pos': 0.075, 'com...</td>\n",
       "      <td>0.3604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   created_utc  \\\n",
       "0           0  1.697508e+09   \n",
       "1           1  1.697501e+09   \n",
       "2           2  1.697499e+09   \n",
       "3           3  1.697498e+09   \n",
       "4           4  1.697498e+09   \n",
       "\n",
       "                                               title  \\\n",
       "0                                            Sierras   \n",
       "1  Car camping recommendations in the GA/NC/TN area?   \n",
       "2                                   Oceano Dunes, CA   \n",
       "3                                   Fort pickens, Fl   \n",
       "4                                     Camping groups   \n",
       "\n",
       "                                           self_text  subreddit  word_count  \\\n",
       "0                                                             0           1   \n",
       "1  Hi there! Looking for solid car camping recomm...          0          30   \n",
       "2  Annual trip with friends & family. Always an e...          0          10   \n",
       "3  Picks from this past weekend. Would recommend ...          0          10   \n",
       "4  Are there camping groups out there, that someo...          0         276   \n",
       "\n",
       "   word_count_title                                          sentiment  \\\n",
       "0                 1  {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...   \n",
       "1                 7  {'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'comp...   \n",
       "2                 3  {'neg': 0.0, 'neu': 0.721, 'pos': 0.279, 'comp...   \n",
       "3                 3  {'neg': 0.0, 'neu': 0.576, 'pos': 0.424, 'comp...   \n",
       "4                 2  {'neg': 0.07, 'neu': 0.855, 'pos': 0.075, 'com...   \n",
       "\n",
       "   vader_score  \n",
       "0       0.0000  \n",
       "1       0.2244  \n",
       "2       0.4767  \n",
       "3       0.7096  \n",
       "4       0.3604  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4ab912cf-ba91-4ef8-bdb5-9b1eebd6e38a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaitl\\AppData\\Local\\Temp\\ipykernel_5660\\3777903337.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df.groupby('subreddit')['word_count', 'vader_score', 'word_count_title'].mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>word_count_title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49.048697</td>\n",
       "      <td>0.260431</td>\n",
       "      <td>9.100137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266.267006</td>\n",
       "      <td>0.594296</td>\n",
       "      <td>8.296673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word_count  vader_score  word_count_title\n",
       "subreddit                                           \n",
       "0           49.048697     0.260431          9.100137\n",
       "1          266.267006     0.594296          8.296673"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('subreddit')['word_count', 'vader_score', 'word_count_title'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241bc6d-b419-46fe-adc3-ac22324e0cb4",
   "metadata": {},
   "source": [
    "### Data Observations\n",
    "\n",
    "1. Posts in the Ultralight subreddit are more positive on average than posts in the camping subreddit \n",
    "2. Posts in the Ultralight subreddit are longer than posts in the camping subreddit\n",
    "\n",
    "\n",
    "#### Implication for Machine Learning:\n",
    "\n",
    "1. The model will be trained on a larger quantity of 'ultralight' vs 'camping' language which could lead to bias\n",
    "2. The culture of the subreddits may be different and not indicative of the larger populations- everyone who is an ultralight backpacker and everyone who camps. The significant difference in sentiment can be due to differences in moderation between the subreddits, rules for posting etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0999db2c-f2db-4c7f-a021-a2ae327b8caf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ultra = df[df['subreddit']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "02a0a526-b2fb-430f-8586-689963e08975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a CountVectorizer for EDA\n",
    "\n",
    "X = ultra['self_text']\n",
    "y = ultra['subreddit']\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "X = cvec.fit_transform(X)\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e86669e7-4450-4177-a0ee-82bf75ac39a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0001', ..., 'östgötaleden', 'оnе', 'расk'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6e5ea777-9036-4420-b545-bb788fea4e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (9438, 1), indices imply (9438, 27030)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X, columns \u001b[38;5;241m=\u001b[39m cvec\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:762\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    754\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    755\u001b[0m             arrays,\n\u001b[0;32m    756\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    759\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    760\u001b[0m         )\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 762\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    763\u001b[0m             data,\n\u001b[0;32m    764\u001b[0m             index,\n\u001b[0;32m    765\u001b[0m             columns,\n\u001b[0;32m    766\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    767\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    768\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    769\u001b[0m         )\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    771\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    772\u001b[0m         {},\n\u001b[0;32m    773\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    776\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    777\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:349\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    345\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    346\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    347\u001b[0m )\n\u001b[1;32m--> 349\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (9438, 1), indices imply (9438, 27030)"
     ]
    }
   ],
   "source": [
    "X_df = pd.DataFrame(X,\n",
    "                    \n",
    "                    columns = cvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5d70fe6a-0a37-4c25-912e-9f5ce384f052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f595a97-1b17-4802-95c3-ab849ecfa66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b759207-5bc4-47cd-a958-2e88427f40bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8b563-6e55-4b4b-857c-0d9468130b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c29c97-5ef5-4ea8-ac1d-bb9152a627c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot top 25 words for each subreddit\n",
    "X_df.sum().sort_values(ascending=False).head(25).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d30487-3ccd-40c2-8260-0ec527052500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I need to remove statuses with links? Or can I just remove the links from the statuses?\n",
    "# Would there be any benefit from leaving them in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d04dbe-6257-456b-b4e2-f921fb76bedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "camp=df[df['subreddit']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc026e-8396-4402-a4cd-c89933c68067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Do I want to add pre-processing to my count-vectorizer such as lemmatizer or tokenizer?\n",
    "\n",
    "X = camp['self_text']\n",
    "y = camp['subreddit']\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "X = cvec.fit_transform(X)\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618841f-5932-4b71-a4c0-e65eda48e749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X.todense(), columns = cvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afc4bd-8b40-4efc-9aee-2955167fa150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot top 25 words for each subreddit\n",
    "X_df.sum().sort_values(ascending=False).head(25).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873f6ec-d79b-4895-8c5a-3beda8c7eff6",
   "metadata": {},
   "source": [
    "When examining the top 25 words from each subreddit, as expected there is some overlap between the two. Words such as looking, tent, day, gear, trip, and time appear frequently in posts in both subreddits. Some of the biggest differences seem to be the frequency of words such as camp and camping vs hike and hiking. I expect that while there will be considerable overlap (in that most words used in one subreddit will also be used in the other) the frequencies of some words will vary greatly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
